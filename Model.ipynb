{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45455448-9f0c-4e0f-9c0e-ced57b7c10a3",
   "metadata": {},
   "source": [
    "# PrimeKG\n",
    "> ```bash\n",
    "> wget -O ./data/kg.csv https://dataverse.harvard.edu/api/access/datafile/6180620\n",
    "> ```\n",
    "- Goal: Given drug node and disease node, score whether an indication edge exists.\n",
    "- Dataset:\n",
    "  - 20 biomedical sources\n",
    "  - 100k+ nodes, 4M+ edges, 29 edge types\n",
    "  - drugâ€“disease indications/contraindications/off-label edges\n",
    "  - fetch kg.csv from Harvard Dataverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a9b06a6-ea53-42bb-9819-3e450d3a5fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries: torch torch-geometric scipy sklearn gdl\n",
    "\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, SAGEConv\n",
    "from torch_geometric.transforms import ToUndirected, RandomLinkSplit\n",
    "from torch_geometric.loader import LinkNeighborLoader\n",
    "\n",
    "# ---- Reproducibility\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "seed_everything(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ---- Configuration\n",
    "CFG = {\n",
    "    \"KG_CSV\": \"./data/kg.csv\",                 # <-- path to PrimeKG kg.csv\n",
    "    \"EMBED_DIM\": 128,\n",
    "    \"HIDDEN\": 128,\n",
    "    \"OUT_DIM\": 128,\n",
    "    \"LR\": 1e-3,\n",
    "    \"WEIGHT_DECAY\": 1e-4,\n",
    "    \"EPOCHS\": 50,\n",
    "    \"PATIENCE\": 8,                      # early stopping on val AP\n",
    "    \"SCHED_STEP\": 20,                   # step LR after this many epochs\n",
    "    \"SCHED_GAMMA\": 0.5,\n",
    "    \"BATCH_SIZE\": 4096,                 # for mini-batch loader\n",
    "    \"NUM_NEIGHBORS\": [15, 10],          # neighbor sampling fanouts\n",
    "    \"NEG_SAMPLING_RATIO\": 1.0,          # negatives per positive (mini-batch)\n",
    "}\n",
    "\n",
    "# our supervision target edge type:\n",
    "TARGET_ET = ('drug','indication','disease')\n",
    "REVERSE_ET = ('disease','rev_indication','drug')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7462f526",
   "metadata": {},
   "source": [
    "## Load PrimeKG & Build Hetero Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e13c63b3-16c1-41b2-bce6-c994d42f538f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node counts: {'drug': 2068, 'disease': 1937, 'protein': 0, 'gene': 0}\n",
      "Edge types: [('drug', 'indication', 'disease'), ('disease', 'rev_indication', 'drug')]\n"
     ]
    }
   ],
   "source": [
    "# ---- Load edges\n",
    "kg = pd.read_csv(CFG[\"KG_CSV\"], low_memory=False)\n",
    "\n",
    "# normalize columns for robust matching\n",
    "for col in [\"x_type\",\"y_type\",\"relation\",\"display_relation\"]:\n",
    "    if col in kg.columns:\n",
    "        kg[col] = kg[col].astype(str).str.lower()\n",
    "\n",
    "# --- Select positive \"indication\" edges --- #\n",
    "pos = kg[\n",
    "    (kg['x_type']=='drug') &\n",
    "    (kg['y_type']=='disease') &\n",
    "    (kg['display_relation'].str.contains('indication', na=False))\n",
    "][['x_id','y_id']].drop_duplicates()\n",
    "\n",
    "# Build initial node sets from positives\n",
    "drug_ids = pd.Index(sorted(set(pos['x_id'])))\n",
    "disease_ids = pd.Index(sorted(set(pos['y_id'])))\n",
    "\n",
    "# Optionally enrich message passing with extra relations (if available):\n",
    "EXTRA_RELATIONS = [\n",
    "    # (src_type, display_relation_substring, dst_type, canonical_edge_type_name)\n",
    "    ('drug',   None,       'protein', 'acts_on'),         # accept all drug-protein relations\n",
    "    ('gene',   None,       'disease', 'gene_assoc'),      # accept all gene-disease relations\n",
    "    # add more if desired (e.g., pathway membership)\n",
    "]\n",
    "\n",
    "# Build per-type ID maps (we will extend when adding extra relations)\n",
    "type2ids = {\n",
    "    'drug': set(drug_ids),\n",
    "    'disease': set(disease_ids),\n",
    "    'protein': set(),\n",
    "    'gene': set(),\n",
    "}\n",
    "edges_by_type = defaultdict(list)  # maps (stype, etype, dtype) -> list of (src, dst)\n",
    "\n",
    "# Add indication edges:\n",
    "for s,d in pos.itertuples(index=False):\n",
    "    type2ids['drug'].add(s)\n",
    "    type2ids['disease'].add(d)\n",
    "    edges_by_type[('drug','indication','disease')].append((s,d))\n",
    "\n",
    "# Add extra relations if present\n",
    "for stype, rel_substr, dtype, canonical in EXTRA_RELATIONS:\n",
    "    sub = kg[(kg['x_type']==stype) & (kg['y_type']==dtype)].copy()\n",
    "    if rel_substr is not None:\n",
    "        sub = sub[sub['display_relation'].str.contains(rel_substr, na=False)]\n",
    "    # if no filter, we include all relations between stype and dtype\n",
    "    if len(sub)==0:\n",
    "        continue\n",
    "    # Track IDs and edges\n",
    "    for s,d in sub[['x_id','y_id']].drop_duplicates().itertuples(index=False):\n",
    "        type2ids[stype].add(s)\n",
    "        type2ids[dtype].add(d)\n",
    "        edges_by_type[(stype, canonical, dtype)].append((s,d))\n",
    "\n",
    "# Freeze ID maps to list->index mappings\n",
    "ntype2index = {}\n",
    "for ntype, idset in type2ids.items():\n",
    "    ids = sorted(idset)\n",
    "    ntype2index[ntype] = {k:i for i,k in enumerate(ids)}\n",
    "    type2ids[ntype] = pd.Index(ids)  # store as index for ordering\n",
    "\n",
    "# Build HeteroData\n",
    "data = HeteroData()\n",
    "for ntype, ids in type2ids.items():\n",
    "    data[ntype].num_nodes = len(ids)\n",
    "\n",
    "# Fill edges\n",
    "def build_edge_index(pairs, src_map, dst_map):\n",
    "    src = torch.tensor([src_map[s] for s,_ in pairs], dtype=torch.long)\n",
    "    dst = torch.tensor([dst_map[d] for _,d in pairs], dtype=torch.long)\n",
    "    return torch.stack([src, dst], dim=0)\n",
    "\n",
    "for (stype, etype, dtype), pairs in edges_by_type.items():\n",
    "    if len(pairs)==0: \n",
    "        continue\n",
    "    ei = build_edge_index(pairs, ntype2index[stype], ntype2index[dtype])\n",
    "    data[(stype, etype, dtype)].edge_index = ei\n",
    "\n",
    "# Add reverse edges so messages flow both ways\n",
    "data = ToUndirected(merge=False)(data)\n",
    "\n",
    "print(\"Node counts:\", {nt: data[nt].num_nodes for nt in data.node_types})\n",
    "print(\"Edge types:\", data.edge_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1164a2f-f7af-403d-828b-04ecc91ddfd7",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a666b7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Featureless baseline: one learnable embedding per node.\n",
    "    You can later replace per-type entries with real features (same dim) if available.\n",
    "    \"\"\"\n",
    "    def __init__(self, data: HeteroData, embed_dim: int):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.emb = nn.ModuleDict()\n",
    "        for ntype in data.node_types:\n",
    "            num_nodes = data[ntype].num_nodes\n",
    "            self.emb[ntype] = nn.Embedding(num_nodes, embed_dim)\n",
    "            nn.init.xavier_uniform_(self.emb[ntype].weight)\n",
    "\n",
    "    def forward_full(self):\n",
    "        # Used in full-batch training (no loaders)\n",
    "        return {ntype: self.emb[ntype].weight for ntype in self.emb.keys()}\n",
    "\n",
    "    def forward_on_batch(self, batch):\n",
    "        # Used with LinkNeighborLoader; select by local->global mapping (n_id)\n",
    "        x = {}\n",
    "        for ntype in batch.node_types:\n",
    "            n_id = batch[ntype].n_id  # global ids of nodes inside the batch\n",
    "            x[ntype] = self.emb[ntype].weight[n_id]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae03a7d4",
   "metadata": {},
   "source": [
    "## Train, Validation, Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54f3581a",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RandomLinkSplit(\n",
    "    num_val=0.1, num_test=0.1,\n",
    "    edge_types=[TARGET_ET],\n",
    "    rev_edge_types=[REVERSE_ET],\n",
    "    add_negative_train_samples=True,  # negatives for train (full-batch path)\n",
    "    is_undirected=True\n",
    ")\n",
    "train_data, val_data, test_data = splitter(data)\n",
    "\n",
    "# Move to device for full-batch path; mini-batch loaders handle device per-batch.\n",
    "train_data = train_data.to(device)\n",
    "val_data   = val_data.to(device)\n",
    "test_data  = test_data.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fa786a",
   "metadata": {},
   "source": [
    "## Encoder & Heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fc594ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeteroSAGE(nn.Module):\n",
    "    def __init__(self, hidden: int, out_dim: int, edge_types, dropout: float = 0.2, use_bn: bool = False):\n",
    "        super().__init__()\n",
    "        self.dropout = dropout\n",
    "        self.use_bn = use_bn\n",
    "\n",
    "        self.conv1 = HeteroConv(\n",
    "            {et: SAGEConv((-1, -1), hidden) for et in edge_types},\n",
    "            aggr='sum'\n",
    "        )\n",
    "        self.conv2 = HeteroConv(\n",
    "            {et: SAGEConv((-1, -1), out_dim) for et in edge_types},\n",
    "            aggr='sum'\n",
    "        )\n",
    "        if use_bn:\n",
    "            self.bn1 = nn.ModuleDict({nt: nn.BatchNorm1d(hidden) for nt in data.node_types})\n",
    "            self.bn2 = nn.ModuleDict({nt: nn.BatchNorm1d(out_dim) for nt in data.node_types})\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        if self.use_bn:\n",
    "            x_dict = {k: self.bn1[k](v) for k,v in x_dict.items()}\n",
    "        x_dict = {k: F.relu(v) for k, v in x_dict.items()}\n",
    "        x_dict = {k: F.dropout(v, p=self.dropout, training=self.training) for k, v in x_dict.items()}\n",
    "        x_dict = self.conv2(x_dict, edge_index_dict)\n",
    "        if self.use_bn:\n",
    "            x_dict = {k: self.bn2[k](v) for k,v in x_dict.items()}\n",
    "        return x_dict  # dict: ntype -> [num_nodes, out_dim]\n",
    "\n",
    "class EdgePredictor(nn.Module):\n",
    "    \"\"\"Concatenate embeddings of (u,v) then MLP -> logits.\"\"\"\n",
    "    def __init__(self, dim: int):\n",
    "        super().__init__()\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(dim * 2, dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, z_src, z_dst):\n",
    "        return self.mlp(torch.cat([z_src, z_dst], dim=-1)).view(-1)\n",
    "\n",
    "# Optional node-level heads (if you add node-level tasks later)\n",
    "class NodeClassifier(nn.Module):\n",
    "    def __init__(self, dim, num_classes):\n",
    "        super().__init__()\n",
    "        self.head = nn.Linear(dim, num_classes)\n",
    "    def forward(self, z):  # z: [N, dim]\n",
    "        return self.head(z)\n",
    "\n",
    "class NodeRegressor(nn.Module):\n",
    "    def __init__(self, dim, out_dim=1):\n",
    "        super().__init__()\n",
    "        self.head = nn.Linear(dim, out_dim)\n",
    "    def forward(self, z):  # z: [N, dim]\n",
    "        return self.head(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed50af9f",
   "metadata": {},
   "source": [
    "## Fullâ€‘Batch Link Prediction Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfb2e905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss 0.6932 | val AUROC 0.606 | val AP 0.555 | lr 1.0e-03\n",
      "Epoch 005 | loss 0.6919 | val AUROC 0.769 | val AP 0.756 | lr 1.0e-03\n",
      "Epoch 010 | loss 0.6878 | val AUROC 0.837 | val AP 0.839 | lr 1.0e-03\n",
      "Epoch 015 | loss 0.6666 | val AUROC 0.851 | val AP 0.855 | lr 1.0e-03\n",
      "Epoch 020 | loss 0.5964 | val AUROC 0.854 | val AP 0.855 | lr 5.0e-04\n",
      "Epoch 025 | loss 0.5276 | val AUROC 0.858 | val AP 0.855 | lr 5.0e-04\n",
      "Early stopping triggered.\n",
      "TEST AUROC 0.861 | AP 0.861\n",
      "{'disease': torch.Size([1937, 128]), 'drug': torch.Size([2068, 128])}\n"
     ]
    }
   ],
   "source": [
    "# Instantiate modules\n",
    "embeds   = NodeEmbeddings(train_data, CFG[\"EMBED_DIM\"]).to(device)\n",
    "encoder  = HeteroSAGE(hidden=CFG[\"HIDDEN\"], out_dim=CFG[\"OUT_DIM\"],\n",
    "                      edge_types=list(train_data.edge_types), dropout=0.2, use_bn=False).to(device)\n",
    "predictor= EdgePredictor(dim=CFG[\"OUT_DIM\"]).to(device)\n",
    "\n",
    "params = list(embeds.parameters()) + list(encoder.parameters()) + list(predictor.parameters())\n",
    "opt = torch.optim.Adam(params, lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=CFG[\"SCHED_STEP\"], gamma=CFG[\"SCHED_GAMMA\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def _get_edge_batch(split_data, et):\n",
    "    ei = split_data[et].edge_label_index\n",
    "    y  = split_data[et].edge_label.float()\n",
    "    return ei, y\n",
    "\n",
    "def forward_once(split_data):\n",
    "    x_dict = embeds.forward_full()\n",
    "    z_dict = encoder(x_dict, split_data.edge_index_dict)\n",
    "    ei, y  = _get_edge_batch(split_data, TARGET_ET)\n",
    "    src, dst = ei[0], ei[1]\n",
    "    z_src = z_dict[TARGET_ET[0]][src]   # 'drug'\n",
    "    z_dst = z_dict[TARGET_ET[2]][dst]   # 'disease'\n",
    "    logits = predictor(z_src, z_dst)\n",
    "    return logits, y\n",
    "\n",
    "def evaluate(split_data):\n",
    "    encoder.eval(); embeds.eval(); predictor.eval()\n",
    "    with torch.no_grad():\n",
    "        logits, y = forward_once(split_data)\n",
    "        prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        y_np = y.detach().cpu().numpy()\n",
    "        return roc_auc_score(y_np, prob), average_precision_score(y_np, prob)\n",
    "\n",
    "best_val_ap, patience, best = -1.0, CFG[\"PATIENCE\"], None\n",
    "\n",
    "# ---- Training loop (full-batch)\n",
    "for epoch in range(1, CFG[\"EPOCHS\"] + 1):\n",
    "    encoder.train(); embeds.train(); predictor.train()\n",
    "    opt.zero_grad()\n",
    "\n",
    "    logits, y = forward_once(train_data)\n",
    "    loss = loss_fn(logits, y)\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(params, 5.0)\n",
    "    opt.step(); sched.step()\n",
    "\n",
    "    # Evaluate\n",
    "    val_auroc, val_ap = evaluate(val_data)\n",
    "    if val_ap > best_val_ap:\n",
    "        best_val_ap = val_ap\n",
    "        patience = CFG[\"PATIENCE\"]\n",
    "        best = {\n",
    "            'encoder': encoder.state_dict(),\n",
    "            'predictor': predictor.state_dict(),\n",
    "        }\n",
    "    else:\n",
    "        patience -= 1\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"Epoch {epoch:03d} | loss {loss.item():.4f} | val AUROC {val_auroc:.3f} | val AP {val_ap:.3f} | lr {sched.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "    if patience == 0:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "# Test with best\n",
    "if best is not None:\n",
    "    encoder.load_state_dict(best['encoder'])\n",
    "    predictor.load_state_dict(best['predictor'])\n",
    "\n",
    "test_auroc, test_ap = evaluate(test_data)\n",
    "print(f\"TEST AUROC {test_auroc:.3f} | AP {test_ap:.3f}\")\n",
    "\n",
    "# Export a sanity check of embedding shapes\n",
    "with torch.no_grad():\n",
    "    z_dict = encoder(embeds.forward_full(), train_data.edge_index_dict)\n",
    "    print({k: v.shape for k,v in z_dict.items()})  # e.g., {'drug': [N_d, 128], 'disease': [N_dis, 128], ...}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f96b7a",
   "metadata": {},
   "source": [
    "## Miniâ€‘Batch Link Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "121eee84",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 81\u001b[39m\n\u001b[32m     78\u001b[39m best_val_ap_mb, patience_mb, best_mb = -\u001b[32m1.0\u001b[39m, CFG[\u001b[33m\"\u001b[39m\u001b[33mPATIENCE\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, CFG[\u001b[33m\"\u001b[39m\u001b[33mEPOCHS\u001b[39m\u001b[33m\"\u001b[39m] + \u001b[32m1\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m     train_loss, _, _ = \u001b[43mrun_epoch_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     82\u001b[39m     val_loss, val_auroc, val_ap = run_epoch_loader(val_loader, train_mode=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     83\u001b[39m     sched_mb.step()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 38\u001b[39m, in \u001b[36mrun_epoch_loader\u001b[39m\u001b[34m(loader, train_mode)\u001b[39m\n\u001b[32m     36\u001b[39m all_probs, all_labels = [], []\n\u001b[32m     37\u001b[39m total_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_mode\u001b[49m\u001b[43m:\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:734\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    731\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    732\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    733\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m734\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    735\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    736\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    737\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    739\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    740\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:790\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    788\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    789\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m790\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    791\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    792\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/fetch.py:55\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch_geometric/loader/link_loader.py:211\u001b[39m, in \u001b[36mLinkLoader.collate_fn\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    208\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Samples a subgraph from a batch of input edges.\"\"\"\u001b[39;00m\n\u001b[32m    209\u001b[39m input_data: EdgeSamplerInput = \u001b[38;5;28mself\u001b[39m.input_data[index]\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlink_sampler\u001b[49m\u001b[43m.\u001b[49m\u001b[43msample_from_edges\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.filter_per_worker:  \u001b[38;5;66;03m# Execute `filter_fn` in the worker process\u001b[39;00m\n\u001b[32m    215\u001b[39m     out = \u001b[38;5;28mself\u001b[39m.filter_fn(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch_geometric/sampler/neighbor_sampler.py:334\u001b[39m, in \u001b[36mNeighborSampler.sample_from_edges\u001b[39m\u001b[34m(self, inputs, neg_sampling)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msample_from_edges\u001b[39m(\n\u001b[32m    330\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    331\u001b[39m     inputs: EdgeSamplerInput,\n\u001b[32m    332\u001b[39m     neg_sampling: Optional[NegativeSampling] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    333\u001b[39m ) -> Union[SamplerOutput, HeteroSamplerOutput]:\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m     out = \u001b[43medge_sample\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisjoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m                      \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnode_time\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_sampling\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.subgraph_type == SubgraphType.bidirectional:\n\u001b[32m    337\u001b[39m         out = out.to_bidirectional()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch_geometric/sampler/neighbor_sampler.py:666\u001b[39m, in \u001b[36medge_sample\u001b[39m\u001b[34m(inputs, sample_fn, num_nodes, disjoint, node_time, neg_sampling)\u001b[39m\n\u001b[32m    661\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m edge_label_time \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# Always disjoint.\u001b[39;00m\n\u001b[32m    662\u001b[39m         seed_time_dict = {\n\u001b[32m    663\u001b[39m             input_type[\u001b[32m0\u001b[39m]: torch.cat([src_time, dst_time], dim=\u001b[32m0\u001b[39m),\n\u001b[32m    664\u001b[39m         }\n\u001b[32m--> \u001b[39m\u001b[32m666\u001b[39m out = \u001b[43msample_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed_time_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    668\u001b[39m \u001b[38;5;66;03m# Enhance `out` by label information ##################################\u001b[39;00m\n\u001b[32m    669\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m disjoint:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/HarryYang/STEMProject/CompBio-PrecisionMed/.venv/lib/python3.13/site-packages/torch_geometric/sampler/neighbor_sampler.py:431\u001b[39m, in \u001b[36mNeighborSampler._sample\u001b[39m\u001b[34m(self, seed, seed_time, **kwargs)\u001b[39m\n\u001b[32m    428\u001b[39m     num_sampled_nodes = num_sampled_edges = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    430\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m431\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m requires \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    432\u001b[39m                       \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33meither \u001b[39m\u001b[33m'\u001b[39m\u001b[33mpyg-lib\u001b[39m\u001b[33m'\u001b[39m\u001b[33m or \u001b[39m\u001b[33m'\u001b[39m\u001b[33mtorch-sparse\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_sampled_edges \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    435\u001b[39m     num_sampled_edges = remap_keys(\n\u001b[32m    436\u001b[39m         num_sampled_edges,\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.to_edge_type,\n\u001b[32m    438\u001b[39m     )\n",
      "\u001b[31mImportError\u001b[39m: 'NeighborSampler' requires either 'pyg-lib' or 'torch-sparse'"
     ]
    }
   ],
   "source": [
    "# ---- Build loaders only if you want mini-batch path\n",
    "# We create loaders for train/val/test on the TARGET_ET.\n",
    "# For train, we let the loader create negatives (neg_sampling_ratio). For val/test, do the same for consistent metrics.\n",
    "\n",
    "def make_loader(split_data, batch_size, num_neighbors, shuffle, neg_sampling_ratio):\n",
    "    return LinkNeighborLoader(\n",
    "        data=split_data,\n",
    "        num_neighbors=num_neighbors,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        edge_label_index=(TARGET_ET, split_data[TARGET_ET].edge_label_index),\n",
    "        edge_label=split_data[TARGET_ET].edge_label if split_data is not train_data else None,\n",
    "        neg_sampling_ratio=neg_sampling_ratio\n",
    "    )\n",
    "\n",
    "train_loader = make_loader(train_data.cpu(), CFG[\"BATCH_SIZE\"], CFG[\"NUM_NEIGHBORS\"], True,  CFG[\"NEG_SAMPLING_RATIO\"])\n",
    "val_loader   = make_loader(val_data.cpu(),   CFG[\"BATCH_SIZE\"], CFG[\"NUM_NEIGHBORS\"], False, CFG[\"NEG_SAMPLING_RATIO\"])\n",
    "test_loader  = make_loader(test_data.cpu(),  CFG[\"BATCH_SIZE\"], CFG[\"NUM_NEIGHBORS\"], False, CFG[\"NEG_SAMPLING_RATIO\"])\n",
    "\n",
    "# Fresh models for mini-batch experiment (to compare apples-to-apples)\n",
    "embeds_mb   = NodeEmbeddings(data, CFG[\"EMBED_DIM\"]).to(device)\n",
    "encoder_mb  = HeteroSAGE(CFG[\"HIDDEN\"], CFG[\"OUT_DIM\"], list(data.edge_types), dropout=0.2, use_bn=False).to(device)\n",
    "predictor_mb= EdgePredictor(CFG[\"OUT_DIM\"]).to(device)\n",
    "\n",
    "params_mb = list(embeds_mb.parameters()) + list(encoder_mb.parameters()) + list(predictor_mb.parameters())\n",
    "opt_mb = torch.optim.Adam(params_mb, lr=CFG[\"LR\"], weight_decay=CFG[\"WEIGHT_DECAY\"])\n",
    "sched_mb = torch.optim.lr_scheduler.StepLR(opt_mb, step_size=CFG[\"SCHED_STEP\"], gamma=CFG[\"SCHED_GAMMA\"])\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def run_epoch_loader(loader, train_mode=True):\n",
    "    if train_mode:\n",
    "        encoder_mb.train(); embeds_mb.train(); predictor_mb.train()\n",
    "    else:\n",
    "        encoder_mb.eval(); embeds_mb.eval(); predictor_mb.eval()\n",
    "\n",
    "    all_probs, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "    for batch in loader:\n",
    "        batch = batch.to(device)\n",
    "        if train_mode:\n",
    "            opt_mb.zero_grad()\n",
    "\n",
    "        # Build mini-batch features from global embeddings via n_id\n",
    "        x_dict = embeds_mb.forward_on_batch(batch)\n",
    "        z_dict = encoder_mb(x_dict, batch.edge_index_dict)\n",
    "\n",
    "        # edge labels are on the batch object (pos + sampled negs)\n",
    "        ei = batch[TARGET_ET].edge_label_index\n",
    "        y  = batch[TARGET_ET].edge_label.float()\n",
    "\n",
    "        src, dst = ei[0], ei[1]\n",
    "        z_src = z_dict[TARGET_ET[0]][src]\n",
    "        z_dst = z_dict[TARGET_ET[2]][dst]\n",
    "        logits = predictor_mb(z_src, z_dst)\n",
    "        loss = loss_fn(logits, y)\n",
    "\n",
    "        if train_mode:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(params_mb, 5.0)\n",
    "            opt_mb.step()\n",
    "\n",
    "        total_loss += loss.item() * y.numel()\n",
    "        prob = torch.sigmoid(logits).detach().cpu().numpy()\n",
    "        lab  = y.detach().cpu().numpy()\n",
    "        all_probs.append(prob); all_labels.append(lab)\n",
    "\n",
    "    if not train_mode:\n",
    "        # compute metrics across all mini-batches\n",
    "        probs = np.concatenate(all_probs, axis=0)\n",
    "        labs  = np.concatenate(all_labels, axis=0)\n",
    "        auroc = roc_auc_score(labs, probs)\n",
    "        ap    = average_precision_score(labs, probs)\n",
    "    else:\n",
    "        auroc = ap = float('nan')\n",
    "    mean_loss = total_loss / max(1, sum(len(a) for a in all_labels))\n",
    "    return mean_loss, auroc, ap\n",
    "\n",
    "best_val_ap_mb, patience_mb, best_mb = -1.0, CFG[\"PATIENCE\"], None\n",
    "\n",
    "for epoch in range(1, CFG[\"EPOCHS\"] + 1):\n",
    "    train_loss, _, _ = run_epoch_loader(train_loader, train_mode=True)\n",
    "    val_loss, val_auroc, val_ap = run_epoch_loader(val_loader, train_mode=False)\n",
    "    sched_mb.step()\n",
    "\n",
    "    if val_ap > best_val_ap_mb:\n",
    "        best_val_ap_mb = val_ap\n",
    "        patience_mb = CFG[\"PATIENCE\"]\n",
    "        best_mb = {\n",
    "            'encoder': encoder_mb.state_dict(),\n",
    "            'predictor': predictor_mb.state_dict(),\n",
    "            'embeds': embeds_mb.state_dict(),  # keep if you want to resume MB training later\n",
    "        }\n",
    "    else:\n",
    "        patience_mb -= 1\n",
    "\n",
    "    if epoch % 5 == 0 or epoch == 1:\n",
    "        print(f\"[MB] Epoch {epoch:03d} | train loss {train_loss:.4f} | val AUROC {val_auroc:.3f} | val AP {val_ap:.3f} | lr {sched_mb.get_last_lr()[0]:.1e}\")\n",
    "\n",
    "    if patience_mb == 0:\n",
    "        print(\"[MB] Early stopping.\")\n",
    "        break\n",
    "\n",
    "# Test with best\n",
    "if best_mb is not None:\n",
    "    encoder_mb.load_state_dict(best_mb['encoder'])\n",
    "    predictor_mb.load_state_dict(best_mb['predictor'])\n",
    "\n",
    "test_loss, test_auroc_mb, test_ap_mb = run_epoch_loader(test_loader, train_mode=False)\n",
    "print(f\"[MB] TEST AUROC {test_auroc_mb:.3f} | AP {test_ap_mb:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759c4fdb",
   "metadata": {},
   "source": [
    "## Save & Load for Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eefa57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Save (full-batch or mini-batch encoder; use the one you prefer)\n",
    "torch.save(encoder.state_dict(), \"./data/primekg_hetero_encoder.pt\")\n",
    "\n",
    "# ---- Load elsewhere\n",
    "# enc_transfer = HeteroSAGE(\n",
    "#     hidden=CFG[\"HIDDEN\"], out_dim=CFG[\"OUT_DIM\"],\n",
    "#     edge_types=list(data.edge_types), dropout=0.2, use_bn=False\n",
    "# ).to(device)\n",
    "# enc_transfer.load_state_dict(torch.load(\"./data/primekg_hetero_encoder.pt\", map_location=device))\n",
    "# enc_transfer.eval()\n",
    "\n",
    "# Example: export embeddings now (from current graph)\n",
    "# with torch.no_grad():\n",
    "#     z_dict = enc_transfer(embeds.forward_full(), data.to(device).edge_index_dict)\n",
    "#     print({k: v.shape for k,v in z_dict.items()})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24244130",
   "metadata": {},
   "source": [
    "## Export Node Embeddings to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eba1c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "drug_index = type2ids.get('drug', pd.Index([]))\n",
    "disease_index = type2ids.get('disease', pd.Index([]))\n",
    "\n",
    "with torch.no_grad():\n",
    "    z_dict = encoder(embeds.forward_full(), data.edge_index_dict)\n",
    "\n",
    "def save_embeddings(ntype, index, z):\n",
    "    if len(index) == 0: return\n",
    "    arr = z.detach().cpu().numpy()\n",
    "    cols = [f\"z{i}\" for i in range(arr.shape[1])]\n",
    "    df = pd.DataFrame(arr, index=list(index), columns=cols)\n",
    "    df.to_csv(f\"emb_{ntype}_{arr.shape[1]}.csv\")\n",
    "\n",
    "save_embeddings('drug', drug_index, z_dict['drug'])\n",
    "save_embeddings('disease', disease_index, z_dict['disease'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24d4a3a",
   "metadata": {},
   "source": [
    "## Temp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add055fe",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
